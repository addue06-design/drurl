import asyncio
import re
import os
import time
from playwright.async_api import async_playwright

# --- è¨­å®šå€åŸŸ ---
TARGET_ID = "202587131"  # ä½ æä¾›çš„æœ€æ–° ID
BASE_DOMAIN = "dramasq.io"
OUTPUT_FILE = "all_episodes_results.txt"
# ------------------

async def get_m3u8_for_ep(page, drama_id, ep):
    results = []
    play_url = f"https://{BASE_DOMAIN}/vodplay/{drama_id}/ep{ep}.html"
    
    try:
        print(f"ğŸ¬ æ­£åœ¨è§£æç¬¬ {ep} é›†: {play_url}")
        # åªéœ€è¦åŠ è¼‰ DOM å³å¯ï¼Œä¸ç”¨ç­‰ç¶²è·¯é–’ç½®
        await page.goto(play_url, wait_until="domcontentloaded", timeout=60000)
        
        # ä½¿ç”¨ JavaScript ç›´æ¥æå–æ‰€æœ‰åŒ…å« v_data çš„ a æ¨™ç±¤è³‡è¨Š
        sources = await page.evaluate("""
            () => {
                const links = Array.from(document.querySelectorAll('a[v_data]'));
                return links.map(a => ({
                    name: a.querySelector('strong') ? a.querySelector('strong').innerText : 'æœªçŸ¥ç‰‡æº',
                    cloud: a.querySelector('small') ? a.querySelector('small').innerText : '',
                    url: a.getAttribute('v_data')
                }));
            }
        """)
        
        for s in sources:
            results.append(f"{s['name']}({s['cloud']}): {s['url']}")
            
    except Exception as e:
        print(f"âš ï¸ ç¬¬ {ep} é›†è§£æç•°å¸¸: {e}")
        
    return results

async def run():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
        page = await context.new_page()

        # 1. åµæ¸¬ç¸½é›†æ•¸
        detail_url = f"https://{BASE_DOMAIN}/detail/{TARGET_ID}.html"
        print(f"ğŸ“¡ åµæ¸¬è©³æƒ…é : {detail_url}")
        await page.goto(detail_url, wait_until="domcontentloaded")
        
        # æå–æ‰€æœ‰ ep{N}.html çš„æ•¸å­—
        content = await page.content()
        ep_numbers = [int(n) for n in re.findall(rf"/{TARGET_ID}/ep(\d+)\.html", content)]
        
        if not ep_numbers:
            # å‚™ç”¨æ–¹æ¡ˆï¼šå¦‚æœæ²’æŠ“åˆ°ï¼Œå˜—è©¦ç›´æ¥å¾æ’­æ”¾æŒ‰éˆ•æ–‡å­—æŠ“
            ep_links = await page.query_selector_all("a[href*='/ep']")
            for link in ep_links:
                text = await link.inner_text()
                num = re.search(r'(\d+)', text)
                if num: ep_numbers.append(int(num.group(1)))

        if not ep_numbers:
            print("âŒ æ‰¾ä¸åˆ°é›†æ•¸åˆ—è¡¨"); await browser.close(); return

        ep_list = sorted(list(set(ep_numbers)))
        print(f"âœ… åµæ¸¬æˆåŠŸï¼šå…± {len(ep_list)} é›† (æ¸…å–®: {ep_list})")

        # 2. å…¨é‡è¦†è“‹å¯«å…¥
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            f.write(f"--- åŠ‡é›† ID: {TARGET_ID} å…¨é‡åŒæ­¥ ({time.strftime('%Y-%m-%d %H:%M:%S')}) ---\n")

        # 3. é€é›†æå–æ‰€æœ‰ç‰‡æº
        for ep in ep_list:
            sources = await get_m3u8_for_ep(page, TARGET_ID, ep)
            
            with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
                f.write(f"\nç¬¬ {ep} é›† (å…± {len(sources)} å€‹ç‰‡æº):\n")
                if sources:
                    for s in sources:
                        f.write(f"  - {s}\n")
                else:
                    f.write("  - (æœªæ‰¾åˆ°ç‰‡æº)\n")
            
            print(f"ğŸ’¾ ç¬¬ {ep} é›†å®Œæˆï¼ŒæŠ“åˆ° {len(sources)} å€‹ç‰‡æº")
            await asyncio.sleep(1) # é€™ç¨®æ–¹æ³•å¾ˆå¿«ï¼Œä¸éœ€è¦ç­‰å¤ªä¹…

        await browser.close()
        print(f"ğŸ ä»»å‹™å®Œæˆï¼çµæœå·²å­˜è‡³ {OUTPUT_FILE}")

if __name__ == "__main__":
    asyncio.run(run())
