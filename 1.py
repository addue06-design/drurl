import asyncio
import re
from playwright.async_api import async_playwright

# --- è¨­å®šå€åŸŸ ---
TARGET_DRAMA_NAME = "é¢¨é›¨æ½®"  # æƒ³è¦æŠ“å–å½±ç‰‡ç¶²å€çš„åŠ‡å
TARGET_EP = 23              # æƒ³è¦æŠ“å–çš„é›†æ•¸
# ----------------

async def run_integrated_scraper():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport={'width': 1280, 'height': 720}
        )
        page = await context.new_page()

        # --- ç¬¬ä¸€éšæ®µï¼šæå–åŠ‡é›†æ¸…å–® ---
        list_url = "https://dramasq.io/type-tv/cn/"
        print(f"ğŸ“‚ æ­£åœ¨æƒææ¸…å–®ç²å–ä»£ç¢¼: {list_url}")
        
        drama_map = {} # ç”¨ä¾†å­˜æ”¾ {åŠ‡å: ä»£ç¢¼}
        try:
            await page.goto(list_url, wait_until="domcontentloaded", timeout=60000)
            await asyncio.sleep(5)
            
            links = await page.query_selector_all("a")
            for link in links:
                href = await link.get_attribute("href")
                title = await link.get_attribute("title")
                text = await link.inner_text()
                
                # åŒ¹é… /detail/æ•¸å­—.html
                match = re.search(r'/detail/(\d+)\.html', href or "")
                if match:
                    d_id = match.group(1)
                    d_name = (title or text).strip()
                    if d_name and not d_name.isdigit() and "EP" not in d_name:
                        drama_map[d_name] = d_id
            
            # å„²å­˜æ¸…å–®åˆ°æª”æ¡ˆ
            with open("all_dramas.txt", "w", encoding="utf-8") as f:
                for name, d_id in drama_map.items():
                    f.write(f"{name} : {d_id}\n")
            print(f"âœ… å·²æˆåŠŸæ›´æ–°åŠ‡é›†æ¸…å–®ï¼Œå…± {len(drama_map)} éƒ¨ã€‚")

        except Exception as e:
            print(f"âŒ æ¸…å–®æƒæå‡ºéŒ¯: {e}")

        # --- ç¬¬äºŒéšæ®µï¼šæå–æŒ‡å®šåŠ‡é›†çš„å½±ç‰‡ç¶²å€ ---
        # æª¢æŸ¥æ¸…å–®ä¸­æ˜¯å¦æœ‰æˆ‘å€‘æƒ³è¦çš„åŠ‡
        target_id = drama_map.get(TARGET_DRAMA_NAME)
        
        if target_id:
            play_url = f"https://dramasq.io/vodplay/{target_id}/ep{TARGET_EP}.html"
            print(f"ğŸš€ å•Ÿå‹•æ·±åº¦è§£æå½±ç‰‡: {play_url}")
            
            m3u8_links = set()
            page.on("request", lambda req: m3u8_links.add(req.url) if ".m3u8" in req.url else None)
            
            try:
                await page.goto(play_url, wait_until="domcontentloaded", timeout=60000)
                
                # æ·±åº¦æƒæ Frame å…§å®¹ (ä½ ä¹‹å‰æˆåŠŸçš„é‚è¼¯)
                for _ in range(15):
                    for frame in page.frames:
                        try:
                            content = await frame.content()
                            found = re.findall(r'https?://[^\s\'"]+\.m3u8[^\s\'"]*', content)
                            for link in found:
                                m3u8_links.add(link)
                        except: continue
                    if m3u8_links: break
                    await asyncio.sleep(1)

                if not m3u8_links:
                    await page.mouse.click(640, 360)
                    await asyncio.sleep(10)

                # å„²å­˜å½±ç‰‡ç¶²å€åˆ°æª”æ¡ˆ
                with open("video_results.txt", "w", encoding="utf-8") as f:
                    if m3u8_links:
                        f.write(f"ã€{TARGET_DRAMA_NAME}ã€‘ç¬¬ {TARGET_EP} é›† m3u8 ç¶²å€:\n")
                        for link in m3u8_links:
                            f.write(f"{link}\n")
                    else:
                        f.write(f"âŒ æœªèƒ½æ‰¾åˆ° ã€{TARGET_DRAMA_NAME}ã€‘ çš„å½±ç‰‡ç¶²å€ã€‚")
                print(f"âœ… å½±ç‰‡ç¶²å€è§£æå®Œæˆï¼Œè«‹æŸ¥çœ‹ video_results.txt")

            except Exception as e:
                print(f"âŒ å½±ç‰‡è§£æå‡ºéŒ¯: {e}")
        else:
            print(f"âš ï¸ åœ¨æ¸…å–®ä¸­æ‰¾ä¸åˆ°åŠ‡å: {TARGET_DRAMA_NAME}")

        await browser.close()

if __name__ == "__main__":
    asyncio.run(run_integrated_scraper())
