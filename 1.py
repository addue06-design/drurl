import asyncio
import re
import os
import random
from playwright.async_api import async_playwright

# --- è¨­å®šå€åŸŸ ---
TARGET_ID = "5597942" 
BASE_DOMAIN = "dramaq.xyz"
OUTPUT_FILE = "all_episodes_results.txt"
# ------------------

async def get_m3u8_for_ep(page, drama_id, ep):
    m3u8_links = set()
    # æ’­æ”¾é ç¶²å€ï¼š/cn/5597942/ep1.html
    play_url = f"https://{BASE_DOMAIN}/cn/{drama_id}/ep{ep}.html"
    
    def handle_request(req):
        if ".m3u8" in req.url.lower():
            m3u8_links.add(req.url)
    page.on("request", handle_request)

    try:
        print(f"ğŸ¬ æ­£åœ¨å…¨é‡è§£æç¬¬ {ep} é›†: {play_url}")
        # ä½¿ç”¨ networkidle ç¢ºä¿éåŒæ­¥çš„å½±ç‰‡è«‹æ±‚æœ‰æ©Ÿæœƒè¢«æ””æˆª
        await page.goto(play_url, wait_until="networkidle", timeout=60000)
        
        # è¼ªè©¢æƒææ‰€æœ‰ frames å…§çš„å…§å®¹
        for _ in range(12):
            if m3u8_links: break
            for frame in page.frames:
                try:
                    content = await frame.content()
                    found = re.findall(r'https?://[^\s\'"]+\.m3u8[^\s\'"]*', content)
                    for link in found: m3u8_links.add(link)
                except: continue
            await asyncio.sleep(2)
            
    except Exception as e:
        print(f"âš ï¸ ç¬¬ {ep} é›†è§£æç•°å¸¸: {e}")
    finally:
        page.remove_listener("request", handle_request)
    return list(m3u8_links)

async def run():
    async with async_playwright() as p:
        # å•Ÿå‹•åƒæ•¸ï¼šéš±è—è‡ªå‹•åŒ–ç‰¹å¾µ
        browser = await p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled"])
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
        )
        page = await context.new_page()

        # 1. åµæ¸¬è©³æƒ…é ç²å–æœ€æ–°é›†æ•¸æ¸…å–®
        detail_url = f"https://{BASE_DOMAIN}/cn/{TARGET_ID}/"
        print(f"ğŸ“¡ å‰å¾€è©³æƒ…é åµæ¸¬æ¸…å–®: {detail_url}")
        
        await page.goto(detail_url, wait_until="domcontentloaded")
        await asyncio.sleep(3) # ç­‰å¾…åˆ—è¡¨åŠ è¼‰

        all_eps = set()
        hrefs = await page.evaluate("() => Array.from(document.querySelectorAll('a')).map(a => a.href)")
        pattern = rf"/{TARGET_ID}/ep(\d+)\.html"
        
        for href in hrefs:
            match = re.search(pattern, href)
            if match:
                all_eps.add(int(match.group(1)))

        if not all_eps:
            # å‚™ç”¨æš´åŠ›æœå°‹
            content = await page.content()
            all_eps.update([int(m) for m in re.findall(pattern, content)])

        if not all_eps:
            print("âŒ ç„¡æ³•åµæ¸¬åˆ°ä»»ä½•é›†æ•¸ã€‚")
            await browser.close(); return

        ep_list = sorted(list(all_eps))
        print(f"âœ… åµæ¸¬æˆåŠŸï¼šå…± {len(ep_list)} é›†ï¼Œæº–å‚™å…¨é‡è¦†è“‹æŠ“å–...")

        # 2. æ¸…ç©º/åˆå§‹åŒ–è¼¸å‡ºæª”æ¡ˆ (ä½¿ç”¨ 'w' æ¨¡å¼)
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            f.write(f"--- {TARGET_ID} å…¨é‡åŒæ­¥çµæœ ---\n")

        # 3. é€ä¸€æŠ“å–
        for ep in ep_list:
            links = await get_m3u8_for_ep(page, TARGET_ID, ep)
            
            # ä½¿ç”¨ 'a' æ¨¡å¼é€è¡Œå­˜å…¥ï¼Œç¢ºä¿è¬ä¸€ç•¶æ©Ÿä¹Ÿèƒ½ä¿å­˜å·²æŠ“å–çš„å…§å®¹
            with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
                f.write(f"ç¬¬ {ep} é›†: {', '.join(links) if links else 'æœªæ‰¾åˆ°'}\n")
            
            print(f"ğŸ’¾ ç¬¬ {ep} é›†å®Œæˆ")
            # éš¨æ©Ÿå»¶æ™‚ä¿è­·
            await asyncio.sleep(random.uniform(1.5, 3.5))

        await browser.close()
        print(f"ğŸ ä»»å‹™å®Œæˆï¼çµæœå·²è¦†è“‹å„²å­˜è‡³ {OUTPUT_FILE}")

if __name__ == "__main__":
    asyncio.run(run())
